{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfOcr21JW3h/TrLLOm2GL1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flari-gold/IU/blob/main/IU_RedditDataAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Package Installs mit pip\n",
        "!pip install praw\n",
        "!pip install HanTa\n",
        "\n",
        "#Import Modules\n",
        "from google.colab import userdata\n",
        "import praw\n",
        "import collections\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from HanTa import HanoverTagger\n",
        "import string\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "\n",
        "#Paket zur Visualisierung der Ergebnisse\n",
        "!pip install pyLDAvis\n",
        "\n",
        "#Module zur Visualisierung der Ergebnisse\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as pyplot\n"
      ],
      "metadata": {
        "id": "TvJ-SwMePddJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameter für die Anmeldung\n",
        "#Daten der unter https://old.reddit.com/prefs/apps/ generierten Anwendung. Ausreichend für ReadOnly Zugriffe.\n",
        "#Die Daten sind im Google Colab Secrets hinterlegt\n",
        "CLIENT_ID=userdata.get('CLIENT_ID')\n",
        "CLIENT_SECRET=userdata.get('CLIENT_SECRET')\n",
        "\n",
        "#Für Write Zugriffe muss ebenfalls noch der eigene Account angegeben werden, kann in diesem Fall ignoriert werden\n",
        "#USERNAME=userdata.get('USERNAME')\n",
        "#PASSWORD=userdata.get('PASSWORD')\n",
        "\n",
        "#Parameter für die Datenanalyse\n",
        "SUBREDDIT='Python'\n",
        "ANZAHL_POSTS=100\n",
        "TOPICS=5\n",
        "PASSES=10\n",
        "COMMENTLIMIT=None"
      ],
      "metadata": {
        "id": "aFH0pne4ODPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Anfrage an die API und Speicherung\n",
        "\n",
        "#Initialisierung reddit API\n",
        "reddit = praw.Reddit(\n",
        "    client_id=CLIENT_ID,\n",
        "    client_secret=CLIENT_SECRET,\n",
        "    user_agent=\"Mein Topic Modeling Script\", #User Agent ist zwingend notwendig\n",
        "    check_for_async=False #PRAW wirft eine Warnung welche die ASYNC API empfiehlt, kann ignoriert werden\n",
        ")\n",
        "\n",
        "#Counter für die Metriken\n",
        "user_upvotes = collections.Counter()\n",
        "user_contributionscounter = collections.Counter()\n",
        "\n",
        "post_numberofcomments = {} #Counter wäre möglich, Dictionary tut es hier auch\n",
        "\n",
        "all_textdata = []\n",
        "#Anfragen an die Reddit API mit über request/http liefern die Daten im JSON Format. PRAW vereinfacht dies, in dem man die direkt die Attribute ansprechen kann\n",
        "for submission in reddit.subreddit(SUBREDDIT).hot(limit=ANZAHL_POSTS):\n",
        "\n",
        "    #Hochzählen der Counter\n",
        "    if submission.author != None:\n",
        "        user_contributionscounter[submission.author] += 1\n",
        "        user_upvotes[submission.author] += submission.score\n",
        "\n",
        "    post_numberofcomments[submission.id] = submission.num_comments\n",
        "\n",
        "    post_textdata = [submission.title]#Alle Reddit Beiträge haben einen Titel. Der Titel wird ebenfalls im Topic Modeling wie ein normaler Kommentar behandelt\n",
        "    post_textdata.append(submission.selftext)#Nicht alle Beträge haben einen Selftext. Beiträge die auf Links verweisen beispielsweise. Falls Selftext vorhanden ist, wird er im Topic Modeling behandelt, ansonsten ist der String leer\n",
        "\n",
        "#Sobald ein Kommntar auf Reddit zu viele Antworten hat, werden die Antworten hinter einer Schaltfläche \"more\" verborgen und das gleiche gilt auch beim Zugriff auf die API.\n",
        "#Das Auflösen der \"more\" stellt einen eigenen API Aufruf da und sorgt aufgrund der Zugriff Limits bei größeren Anfragen zu einer langen Laufzeit\n",
        "    submission.comments.replace_more(limit=COMMENTLIMIT)\n",
        "\n",
        "    for comment in submission.comments.list():\n",
        "\n",
        "        #Hochzählen der Counter\n",
        "        if comment.author != None:\n",
        "            user_contributionscounter[comment.author] += 1\n",
        "            user_upvotes[comment.author] += comment.score\n",
        "\n",
        "        post_textdata.append(comment.body)\n",
        "\n",
        "    all_textdata.append(post_textdata)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZ7Ks5C0Pl3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Bereinigung\n",
        "\n",
        "#Stopwörter von NLTK\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['would', 'get', 'thats', 'im', 'ive', 'dont', 'didnt', 'cant', 'also'])#Stopwords enthält standardmäßig 'i' und 'am' aber weder 'i'm' noch 'im'\n",
        "\n",
        "#Lemmatiserung mit HanoverTagger\n",
        "tagger_en = HanoverTagger.HanoverTagger('morphmodel_en.pgz')\n",
        "\n",
        "#Zu entfernende Satzzeichen\n",
        "punctuation = string.punctuation\n",
        "punctuation += '’–'\n",
        "\n",
        "all_cleantext = []\n",
        "for i in all_textdata:\n",
        "\n",
        "    string_result = []\n",
        "    for j in i:\n",
        "\n",
        "        templist = []\n",
        "        #Entfernung der Satzzeichen\n",
        "        tempstring = j.translate(str.maketrans('', '', punctuation))\n",
        "\n",
        "        #Umwandlung in Kleinbuchstaben\n",
        "        for word in tempstring.lower().split():\n",
        "            #Lemmatisierung\n",
        "            templist.append(tagger_en.analyze(word)[0])\n",
        "        #Filterung von Stopwörtern\n",
        "        templist = [word for word in templist if word not in stop_words]\n",
        "        tempstring = ' '.join(templist)\n",
        "\n",
        "        string_result.append(tempstring)\n",
        "    all_cleantext.append(string_result)\n",
        "\n",
        "#wird später für die Aufbereitung der Ergebnisse genutzt, schreibt alles in einen String\n",
        "templist = []\n",
        "for i in all_cleantext:\n",
        "    long_string = ' '.join(i)\n",
        "    templist.append(long_string)\n",
        "end_string = ' '.join(templist)\n"
      ],
      "metadata": {
        "id": "ptDocgDFQRpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Erstellung des LDA Models mit Hilfe von Gensim\n",
        "\n",
        "\n",
        "#all_cleantext ist eine Liste, welche Listen beinhaltet, welche wiederum Strings beinhalten\n",
        "#Jeder String repräsentiert einen Post/Kommentar\n",
        "#Für die Verarbeitung muss aber in der Liste für jedes Wort ein eigener String stehen\n",
        "templist = []\n",
        "for i in all_cleantext:\n",
        "    long_string = ' '.join(i)\n",
        "    templist.append(long_string.split())\n",
        "\n",
        "\n",
        "allwords = corpora.Dictionary(templist)\n",
        "allposts = [allwords.doc2bow(text) for text in templist]\n",
        "\n",
        "lda_model = LdaModel(allposts, num_topics=TOPICS, id2word=allwords, passes=PASSES)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rdiN6Pv2JWmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ausgabe der Ergebnisse in Textform\n",
        "\n",
        "#Ausgabe der Keywords pro Topic\n",
        "print(\"Analyse des Subreddits: \" + SUBREDDIT)\n",
        "\n",
        "print(\"Gewollte Anzahl an abgefragten Posts: \" + str(ANZAHL_POSTS))\n",
        "print(\"Tatsächlich abgefragt: \" + str(len(post_numberofcomments)))#Falls das Subreddit nicht genug Posts beinhaltet\n",
        "print(\"Durchschnittliche Anzahl an Kommentaren pro Post: \" + str(sum(post_numberofcomments.values())/len(post_numberofcomments)))\n",
        "print(\"Kleinsten Anzahl an Kommentaren pro Post: \" + str(min(post_numberofcomments.values())))\n",
        "print(\"Größte Anzahl an Kommentaren pro Post: \" + str(max(post_numberofcomments.values())))\n",
        "\n",
        "print(\"\\nAnzahl zu ermittelten Topics: \" + str(TOPICS))\n",
        "print(\"Keywords pro ermitteltem Topic\")\n",
        "for t,x in lda_model.print_topics():\n",
        "    print(\"Topic {}: {}\".format(t+1, x))\n",
        "\n",
        "#Häufigste Wörter\n",
        "\n",
        "print(\"\\nHäufigste Wörter\")\n",
        "for t,x in collections.Counter(end_string.split()).most_common(10):\n",
        "    print(\"{}: {}\".format(t, x))\n",
        "\n",
        "#Ausgabe der aktivsten User\n",
        "print(\"\\nMeiste Anzahl Beiträge pro User\")\n",
        "for t,x in user_contributionscounter.most_common(10):\n",
        "    print(\"{}: {}\".format(t, x))\n",
        "\n",
        "#Ausgabe der User mit den meisten Upvotes\n",
        "print(\"\\nMeiste Anzahl Upvotes pro User\")\n",
        "for t,x in user_upvotes.most_common(10):\n",
        "    print(\"{}: {}\".format(t, x))"
      ],
      "metadata": {
        "id": "OTb424IKDeCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ausgabe der Ergebnisse in Grafiken\n",
        "\n",
        "#LDA Model Visualisiert\n",
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.gensim.prepare(lda_model, allposts, allwords)\n",
        "pyLDAvis.display(panel)"
      ],
      "metadata": {
        "id": "9PjkrBj-6GLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wordclouds der ermittelten Topics\n",
        "\n",
        "#Erzeugt eine figure in der die wordclouds einen eigenen Subplot bekommen, Wordclouds überschreiben sich sonst\n",
        "fig, axes = pyplot.subplots(1, len(lda_model.print_topics()), figsize=(5*len(lda_model.print_topics()), 5))\n",
        "\n",
        "for i in lda_model.print_topics():\n",
        "    wordcloud = WordCloud(background_color=\"white\", width=800, height=800, colormap=\"cividis\")\n",
        "    wordcloud.generate(i[1])\n",
        "    axes[i[0]].imshow(wordcloud)\n",
        "    axes[i[0]].axis('off')\n",
        "\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "ZUkA8W_1jBZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wordcloud der häufigsten Wörter\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=50, width=800, height=800, colormap=\"twilight_shifted\")\n",
        "wordcloud.generate(end_string)\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "O1Vx8nFxTXCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#Diagramme der aktivsten und meist hochghewählten Nutzer\n",
        "\n",
        "#Sortiere die Nutzer nach Anzahl an Beiträgen\n",
        "top_contributors = sorted(user_contributionscounter.items(), key=lambda item: item[1], reverse=True)[:10]\n",
        "\n",
        "#Extrahiere Namen und Anzahl an Beiträgen der Top 10 Redditor\n",
        "contributor = [str(c[0].name) for c in top_contributors]\n",
        "contributions = [c[1] for c in top_contributors]\n",
        "\n",
        "#Erstelle Diagramm\n",
        "pyplot.bar(contributor, contributions)\n",
        "pyplot.xlabel(\"Redditor\")\n",
        "pyplot.ylabel(\"Anzahl an Beiträgen\")\n",
        "pyplot.title(\"Top 10 aktivste Redditor\")\n",
        "pyplot.xticks(rotation=90)\n",
        "pyplot.show()\n",
        "\n",
        "\n",
        "#Sortiere die Nutzer nach Anzahl an Upvotes\n",
        "top_contributors = sorted(user_upvotes.items(), key=lambda item: item[1], reverse=True)[:10]\n",
        "\n",
        "#Extrahiere Namen und Anzahl an Upvotes der Top 10 Redditor\n",
        "contributor = [str(c[0].name) for c in top_contributors]\n",
        "upvotes = [c[1] for c in top_contributors]\n",
        "\n",
        "#Erstelle Diagramm\n",
        "pyplot.bar(contributor, upvotes, color ='maroon')\n",
        "pyplot.xlabel(\"Redditor\")\n",
        "pyplot.ylabel(\"Anzahl an Upvotes\")\n",
        "pyplot.title(\"Top 10 meist hochgewählte Redditor\")\n",
        "pyplot.xticks(rotation=90)\n",
        "pyplot.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CE-zJHF5bvwZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}