{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPfd+zhLrgpPoLskXXEAjn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flari-gold/IU/blob/main/IU_RedditDataAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Package Installs mit pip\n",
        "!pip install praw\n",
        "!pip install HanTa\n",
        "\n",
        "#Import Modules\n",
        "from google.colab import userdata\n",
        "import praw\n",
        "import collections\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from HanTa import HanoverTagger"
      ],
      "metadata": {
        "id": "TvJ-SwMePddJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameter für die Anmeldung\n",
        "#Daten der unter https://old.reddit.com/prefs/apps/ generierten Anwendung. Ausreichend für ReadOnly Zugriffe.\n",
        "#Die Daten sind im Google Colab Secrets hinterlegt\n",
        "CLIENT_ID=userdata.get('CLIENT_ID')\n",
        "CLIENT_SECRET=userdata.get('CLIENT_SECRET')\n",
        "\n",
        "#Für Write Zugriffe muss ebenfalls noch der eigene Account angegeben werden, kann in diesem Fall ignoriert werden\n",
        "#USERNAME=userdata.get('USERNAME')\n",
        "#PASSWORD=userdata.get('PASSWORD')\n",
        "\n",
        "#Parameter für die Datenanalyse\n",
        "SUBREDDIT='linuxmemes'\n",
        "ANZAHL_POSTS=100\n",
        "TOPICS=5\n",
        "PASSES=10\n",
        "COMMENTLIMIT=None"
      ],
      "metadata": {
        "id": "aFH0pne4ODPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Anfrage an die API und Speicherung\n",
        "\n",
        "#Initialisierung reddit API\n",
        "reddit = praw.Reddit(\n",
        "    client_id=CLIENT_ID,\n",
        "    client_secret=CLIENT_SECRET,\n",
        "    user_agent=\"Mein Topic Modeling Script\", #User Agent ist zwingend notwendig\n",
        "    check_for_async=False #PRAW wirft eine Warnung welche die ASYNC API empfiehlt, kann ignoriert werden\n",
        ")\n",
        "\n",
        "#Counter für die Metriken\n",
        "user_upvotes = collections.Counter()\n",
        "user_contributionscounter = collections.Counter()\n",
        "\n",
        "all_textdata = []\n",
        "#Anfragen an die Reddit API mit über request/http liefern die Daten im JSON Format. PRAW vereinfacht dies, in dem man die direkt die Attribute ansprechen kann\n",
        "for submission in reddit.subreddit(SUBREDDIT).hot(limit=ANZAHL_POSTS):\n",
        "\n",
        "    #Hochzählen der Counter\n",
        "    if submission.author != None:\n",
        "        user_contributionscounter[submission.author] += 1\n",
        "        user_upvotes[submission.author] += submission.score\n",
        "\n",
        "    post_textdata = [submission.title]#Alle Reddit Beiträge haben einen Titel. Der Titel wird ebenfalls im Topic Modeling wie ein normaler Kommentar behandelt\n",
        "    post_textdata.append(submission.selftext)#Nicht alle Beträge haben einen Selftext. Beiträge die auf Links verweisen beispielsweise. Falls Selftext vorhanden ist, wird er im Topic Modeling behandelt, ansonsten ist der String leer\n",
        "\n",
        "#Sobald ein Kommntar auf Reddit zu viele Antworten hat, werden die Antworten hinter einer Schaltfläche \"more\" verborgen und das gleiche gilt auch beim Zugriff auf die API.\n",
        "#Das Auflösen der \"more\" stellt einen eigenen API Aufruf da und sorgt aufgrund der Zugriff Limits bei größeren Anfragen zu einer langen Laufzeit\n",
        "    submission.comments.replace_more(limit=COMMENTLIMIT)\n",
        "\n",
        "    for comment in submission.comments.list():\n",
        "\n",
        "        #Hochzählen der Counter\n",
        "        if comment.author != None:\n",
        "            user_contributionscounter[comment.author] += 1\n",
        "            user_upvotes[comment.author] += comment.score\n",
        "\n",
        "        post_textdata.append(comment.body)\n",
        "\n",
        "    all_textdata.append(post_textdata)\n",
        "\n",
        "#print(user_contributionscounter)\n",
        "#print(user_upvotes)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZ7Ks5C0Pl3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Bereinigung\n",
        "\n",
        "#Stopwörter von NLTK\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['would', 'get'])\n",
        "\n",
        "#Lemmatiserung mit HanoverTagger\n",
        "tagger_en = HanoverTagger.HanoverTagger('morphmodel_en.pgz')\n",
        "\n",
        "all_cleantext = []\n",
        "for i in all_textdata:\n",
        "\n",
        "    string_result = []\n",
        "    for j in i:\n",
        "\n",
        "        templist = []\n",
        "        #sehr unschöne Lösung meiner Meinung nach, Tokenizer sinnvoll\n",
        "        tempstring = j.replace(\".\", \"\").replace(\",\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \" \").replace(\";\", \" \").replace(\":\", \"\").replace(\"'\", \"\").replace(\"’\", \"\").replace(\"@\", \"\").replace(\"+\", \" \").replace(\"-\", \" \").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"*\", \"\").replace(\"–\", \"\").replace(\"/\", \"\") #’–\n",
        "        #Umwandlung in Kleinbuchstaben\n",
        "        for word in tempstring.lower().split():\n",
        "            #Lemmatisierung\n",
        "            templist.append(tagger_en.analyze(word)[0])\n",
        "        #Filterung von Stopwörtern\n",
        "        templist = [word for word in templist if word not in stop_words]\n",
        "        tempstring = ' '.join(templist)\n",
        "\n",
        "        string_result.append(tempstring)\n",
        "    all_cleantext.append(string_result)\n",
        "    #print(string_result)\n",
        "\n"
      ],
      "metadata": {
        "id": "ptDocgDFQRpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Erstellung des LDA Models mit Hilfe von Gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "#all_cleantext ist eine Liste, welche Listen beinhaltet, welche wiederum Strings beinhalten\n",
        "#Jeder String repräsentiert einen Post/Kommentar\n",
        "#Für die Verarbeitung muss aber in der Liste für jedes Wort ein eigener String stehen\n",
        "templist = []\n",
        "for i in all_cleantext:\n",
        "    long_string = ' '.join(i)\n",
        "    templist.append(long_string.split())\n",
        "\n",
        "allwords = corpora.Dictionary(templist)\n",
        "allposts = [allwords.doc2bow(text) for text in templist]\n",
        "\n",
        "lda_model = LdaModel(allposts, num_topics=TOPICS, id2word=allwords, passes=PASSES)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rdiN6Pv2JWmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ausgabe der Ergebnisse\n",
        "\n",
        "#Ausgabe der Keywords pro Topic\n",
        "print(\"Analyse des Subreddits: \" + SUBREDDIT)\n",
        "\n",
        "print(\"\\nKeywords pro ermitteltem Topic\")\n",
        "for t,x in lda_model.print_topics():\n",
        "    print(\"Topic {}: {}\".format(t, x))\n",
        "\n",
        "#Ausgabe der aktivsten User\n",
        "print(\"\\nMeiste Anzahl Beiträge pro User\")\n",
        "for t,x in user_contributionscounter.most_common(10):\n",
        "    print(\"{}: {}\".format(t, x))\n",
        "\n",
        "#Ausgabe der User mit den meisten Upvotes\n",
        "print(\"\\nMeiste Anzahl Upvotes pro User\")\n",
        "for t,x in user_upvotes.most_common(10):\n",
        "    print(\"{}: {}\".format(t, x))"
      ],
      "metadata": {
        "id": "OTb424IKDeCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyse des Subreddits: linuxmemes\n",
        "\n",
        "Keywords pro ermitteltem Topic\n",
        "\n",
        "Topic 0: 0.014*\"use\" + 0.011*\"Linux\" + 0.011*\"arch\" + 0.010*\"like\" + 0.009*\"fedora\" + 0.007*\"dont\" + 0.006*\"nuclear\" + 0.006*\"make\" + 0.006*\"one\" + 0.005*\"say\"\n",
        "\n",
        "Topic 1: 0.011*\"Linux\" + 0.010*\"use\" + 0.008*\"window\" + 0.006*\"make\" + 0.006*\"file\" + 0.005*\"want\" + 0.005*\"french\" + 0.005*\"like\" + 0.004*\"best\" + 0.004*\"good\"\n",
        "\n",
        "Topic 2: 0.021*\"use\" + 0.011*\"arch\" + 0.008*\"like\" + 0.008*\"Linux\" + 0.007*\"dont\" + 0.006*\"one\" + 0.006*\"user\" + 0.006*\"im\" + 0.005*\"work\" + 0.005*\"make\"\n",
        "\n",
        "Topic 3: 0.006*\"comment\" + 0.006*\"use\" + 0.005*\"cat\" + 0.004*\"Linux\" + 0.004*\"post\" + 0.004*\"Meme\" + 0.003*\"Sudo\" + 0.003*\"^^|\" + 0.003*\"like\" + 0.003*\"im\"\n",
        "\n",
        "Topic 4: 0.014*\"use\" + 0.011*\"Linux\" + 0.010*\"Ubuntu\" + 0.008*\"like\" + 0.007*\"window\" + 0.007*\"snap\" + 0.007*\"good\" + 0.006*\"distro\" + 0.006*\"dont\" + 0.005*\"make\""
      ],
      "metadata": {
        "id": "w8Vza0XGxOG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyse des Subreddits: ProgrammerHumor\n",
        "\n",
        "Keywords pro ermitteltem Topic\n",
        "\n",
        "Topic 0: 0.014*\"use\" + 0.009*\"like\" + 0.008*\"count\" + 0.007*\"language\" + 0.007*\"length\" + 0.007*\"userid\" + 0.006*\"dont\" + 0.006*\"array\" + 0.006*\"make\" + 0.006*\"size\"\n",
        "\n",
        "Topic 1: 0.016*\"c\" + 0.015*\"use\" + 0.014*\"tailwin\" + 0.009*\"like\" + 0.008*\"style\" + 0.008*\"class\" + 0.008*\"component\" + 0.006*\"make\" + 0.006*\"good\" + 0.006*\"dont\"\n",
        "\n",
        "Topic 2: 0.010*\"use\" + 0.010*\"work\" + 0.010*\"code\" + 0.008*\"like\" + 0.007*\"make\" + 0.006*\"dont\" + 0.005*\"time\" + 0.005*\"one\" + 0.004*\"ai\" + 0.004*\"good\"\n",
        "\n",
        "Topic 3: 0.018*\"file\" + 0.015*\"git\" + 0.010*\"use\" + 0.009*\"change\" + 0.009*\"work\" + 0.008*\"backup\" + 0.008*\"like\" + 0.007*\"make\" + 0.007*\"dont\" + 0.006*\"discard\"\n",
        "\n",
        "Topic 4: 0.011*\"use\" + 0.011*\"commit\" + 0.009*\"like\" + 0.006*\"dont\" + 0.006*\"one\" + 0.006*\"work\" + 0.006*\"code\" + 0.006*\"make\" + 0.006*\"time\" + 0.006*\"git\""
      ],
      "metadata": {
        "id": "Ub02TV63v5lD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyse des Subreddits: Slipknot\n",
        "\n",
        "Keywords pro ermitteltem Topic\n",
        "\n",
        "Topic 0: 0.012*\"song\" + 0.010*\"like\" + 0.010*\"iowa\" + 0.009*\"album\" + 0.007*\"title\" + 0.007*\"love\" + 0.007*\"one\" + 0.007*\"slipknot\" + 0.007*\"self\" + 0.005*\"dont\"\n",
        "\n",
        "Topic 1: 0.012*\"im\" + 0.010*\"like\" + 0.008*\"thats\" + 0.008*\"slipknot\" + 0.007*\"make\" + 0.007*\"people\" + 0.007*\"think\" + 0.006*\"one\" + 0.006*\"go\" + 0.006*\"corey\"\n",
        "\n",
        "Topic 2: 0.020*\"album\" + 0.017*\"like\" + 0.017*\"song\" + 0.011*\"time\" + 0.011*\"first\" + 0.010*\"see\" + 0.010*\"dont\" + 0.008*\"one\" + 0.008*\"slipknot\" + 0.008*\"good\"\n",
        "\n",
        "Topic 3: 0.012*\"slipknot\" + 0.009*\"one\" + 0.009*\"Sic\" + 0.007*\"song\" + 0.007*\"go\" + 0.006*\"cool\" + 0.006*\"make\" + 0.006*\"good\" + 0.006*\"dont\" + 0.006*\"like\"\n",
        "\n",
        "Topic 4: 0.009*\"version\" + 0.009*\"iowa\" + 0.009*\"self\" + 0.009*\"title\" + 0.008*\"one\" + 0.008*\"corey\" + 0.008*\"mfkr\" + 0.008*\"album\" + 0.008*\"slipknot\" + 0.007*\"cover\""
      ],
      "metadata": {
        "id": "k66e5EiqtsyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Nicht teil der Aufgabe, aber hilft zur Interpretation\n",
        "#Paket zur Visualisierung\n",
        "!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim"
      ],
      "metadata": {
        "id": "afAGKD3U6EwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Erstellt Grafik\n",
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.gensim.prepare(lda_model, allposts, allwords)\n",
        "pyLDAvis.display(panel)"
      ],
      "metadata": {
        "id": "9PjkrBj-6GLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nicht teil der Aufgabe\n",
        "#Auflistung der häufigsten Wörter\n",
        "templist = []\n",
        "for i in all_cleantext:\n",
        "    long_string = ' '.join(i)\n",
        "    templist.append(long_string)\n",
        "end_string = ' '.join(templist)\n",
        "\n",
        "\n",
        "print(collections.Counter(end_string.split()).most_common(10))"
      ],
      "metadata": {
        "id": "wMII6EANwGnT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}