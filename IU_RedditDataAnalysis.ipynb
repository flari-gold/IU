{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdkJMo5yXS95D3z+zNCyuX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flari-gold/IU/blob/main/IU_RedditDataAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#package installs with pip\n",
        "!pip install praw"
      ],
      "metadata": {
        "id": "TvJ-SwMePddJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import modules\n",
        "from google.colab import userdata\n",
        "import praw\n",
        "import collections"
      ],
      "metadata": {
        "id": "5vOxKhQy3p3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter\n",
        "CLIENT_ID=userdata.get('CLIENT_ID')\n",
        "CLIENT_SECRET=userdata.get('CLIENT_SECRET')\n",
        "#PASSWORD=userdata.get('CLIENT_ID')\n",
        "#USERNAME=userdata.get('CLIENT_ID')\n",
        "SUBREDDIT='Sylosis'\n",
        "ANZAHL_POSTS=30"
      ],
      "metadata": {
        "id": "Tk1XFLpT4g4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialisierung reddit API\n",
        "reddit = praw.Reddit(\n",
        "    client_id=CLIENT_ID,\n",
        "    client_secret=CLIENT_SECRET,\n",
        "    user_agent=\"my user agent\",\n",
        "    check_for_async=False #PRAW wirft eine Warnung welche die ASYNC API empfiehlt, kann ignoriert werden\n",
        ")"
      ],
      "metadata": {
        "id": "aFH0pne4ODPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#declaration\n",
        "all_textdata = []\n",
        "user_upvotes = collections.Counter()\n",
        "user_contributionscounter = collections.Counter()\n"
      ],
      "metadata": {
        "id": "WhbpcIBpXkAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kx2Uz2TabHm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Anfragen an die Reddit API mit über request liefern die Daten im JSON Format. PRAW vereinfacht dies, in dem man die direkt die Attribute ansprechen kann\n",
        "for submission in reddit.subreddit(SUBREDDIT).hot(limit=ANZAHL_POSTS):\n",
        "\n",
        "    user_contributionscounter[submission.author] += 1\n",
        "    user_upvotes[submission.author] += submission.score\n",
        "\n",
        "    post_textdata = [submission.title]\n",
        "    post_textdata.append(submission.selftext)\n",
        "\n",
        "\n",
        "    submission.comments.replace_more(limit=None)\n",
        "    for comment in submission.comments.list():\n",
        "        post_textdata.append(comment.body)\n",
        "        user_contributionscounter[comment.author] += 1\n",
        "        user_upvotes[comment.author] += comment.score\n",
        "        #print(comment.body)\n",
        "\n",
        "    #print(post_textdata)\n",
        "    all_textdata.append(post_textdata)\n",
        "    #print(\"----------------------------------------------\")\n",
        "\n",
        "\n",
        "print(user_contributionscounter)\n",
        "print(user_upvotes)\n",
        "#print(all_textdata)"
      ],
      "metadata": {
        "id": "ZZ7Ks5C0Pl3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean text\n",
        "for i in all_textdata:\n",
        "    #print(i)\n",
        "    for j in i:\n",
        "        print(j)\n"
      ],
      "metadata": {
        "id": "ptDocgDFQRpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subreddit = 'finanzen'\n",
        "for submission in reddit.subreddit(subreddit).hot(limit=5):\n",
        "    print(submission.title)\n",
        "    print(submission.id)\n",
        "    submission.comments.replace_more(limit=None)\n",
        "    for comment in submission.comments.list():\n",
        "        print(comment.body)\n",
        "        print(\"----------------------------------------------\")\n",
        "\n",
        "\n",
        "#for submission in reddit.subreddit(\"test\").hot(limit=10):\n",
        "#    print(submission.title)\n",
        "#    print(submission.id)\n",
        "\n",
        "#submission = reddit.submission(\"1gmifua\")\n",
        "#submission.comments.replace_more(limit=None)\n",
        "#for comment in submission.comments.list():\n",
        "#    print(comment.body)\n",
        "#    print(\"----------------------------------------------\")\n",
        "\n",
        "submission = reddit.submission(\"1gmifua\")\n",
        "print(submission.score)\n",
        "comment = reddit.comment(\"lxw66nl\")\n",
        "print(comment.score)"
      ],
      "metadata": {
        "id": "lYA_Yss9V6Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJQsC_DTbylt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QEjiexS9pRqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.linspace(0, 10, 100)\n",
        "\n",
        "plt.plot(x, np.sin(x))\n",
        "plt.plot(x, np.cos(x))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DKG3GTYD8yRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install HanTa\n",
        "from HanTa import HanoverTagger as ht\n",
        "\n",
        "tagger_de = ht.HanoverTagger('morphmodel_ger.pgz')\n",
        "\n"
      ],
      "metadata": {
        "id": "R8RGAmbv166c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tagger_de.analyze('fährt'))"
      ],
      "metadata": {
        "id": "aF3D66-x2TZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "review_1 = \"Loved the sound, no battery issues\"\n",
        "review_2 = \"Sound quality is good; battery life not good\"\n",
        "vectorizer = TfidfVectorizer(use_idf = True,\n",
        "\tmax_features = 5,\n",
        "\tsmooth_idf=True)\n",
        "model = vectorizer.fit_transform([review_1, review_2])\n",
        "\n",
        "lda_model=LatentDirichletAllocation(n_components=2,learning_method=\n",
        "'online',random_state=42,max_iter=1)\n",
        "lda_top=lda_model.fit_transform(model)\n",
        "\n",
        "print(\"Review 1: \")\n",
        "for i,topic in enumerate(lda_top[0]):\n",
        "\tprint(\"Topic \",i,\": \",topic*100,\"%\")\n",
        "\n",
        "for i,topic in enumerate(lda_top[1]):\n",
        "\tprint(\"Topic \",i,\": \",topic*100,\"%\")"
      ],
      "metadata": {
        "id": "u2tBDdMSBgNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "\n",
        "# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
        "auth = requests.auth.HTTPBasicAuth(userdata.get('CLIENT_ID'), userdata.get('CLIENT_SECRET'))\n",
        "\n",
        "# here we pass our login method (password), username, and password\n",
        "data = {'grant_type': 'password',\n",
        "        'username': userdata.get('USERNAME'),\n",
        "        'password': userdata.get('PASSWORD')}\n",
        "\n",
        "# setup our header info, which gives reddit a brief description of our app\n",
        "headers = {'User-Agent': 'MyBot/0.0.1'}\n",
        "\n",
        "# send our request for an OAuth token\n",
        "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
        "                    auth=auth, data=data, headers=headers)\n",
        "\n",
        "# convert response to JSON and pull access_token value\n",
        "TOKEN = res.json()['access_token']\n",
        "\n",
        "# add authorization to our headers dictionary\n",
        "headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
        "\n",
        "# while the token is valid (~2 hours) we just add headers=headers to our requests\n",
        "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)"
      ],
      "metadata": {
        "id": "dMuoORVX3Ob2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subreddit = 'de'\n",
        "limit = 50\n",
        "timeframe = 'all' #hour, day, week, month, year, all\n",
        "listing = 'top' # controversial, best, hot, new, random, rising, top\n",
        "\n",
        "url = f\"https://oauth.reddit.com/r/{subreddit}/{listing}?limit={limit}&t={timeframe}\"\n",
        "res = requests.get(url, headers=headers)\n",
        "\n",
        "import json\n",
        "#print(json.dumps(res.json(), indent=2))\n",
        "\n",
        "\n",
        "\n",
        "def find_values(id, json_repr):\n",
        "    results = []\n",
        "\n",
        "    def _decode_dict(a_dict):\n",
        "        try:\n",
        "            results.append(a_dict[id])\n",
        "        except KeyError:\n",
        "            pass\n",
        "        return a_dict\n",
        "\n",
        "    json.loads(json_repr, object_hook=_decode_dict) # Return value ignored.\n",
        "    return results\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud# Join the different processed titles together.\n",
        "#long_string = ','.join(list(papers['paper_text_processed'].values))# Create a WordCloud object\n",
        "long_string = ' '.join(find_values('title', json.dumps(res.json())))\n",
        "#print(long_string)\n",
        "templist = [word for word in long_string.split() if word.lower() not in stopwords.words('german')]\n",
        "long_string = ' '.join(templist)\n",
        "\n",
        "#\n",
        "\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')# Generate a word cloud\n",
        "wordcloud.generate(long_string)# Visualize the word cloud\n",
        "wordcloud.to_image()\n",
        "\n"
      ],
      "metadata": {
        "id": "S4nOoReL3SwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subreddit = 'finanzen'\n",
        "limit = 5\n",
        "timeframe = 'week' #hour, day, week, month, year, all\n",
        "listing = 'top' # controversial, best, hot, new, random, rising, top\n",
        "id = '1gmifua'\n",
        "\n",
        "#http://www.reddit.com/r/\" + sub + \"/comments/\" + id + \".json?\",\n",
        "url = f\"https://oauth.reddit.com/r/{subreddit}/comments/{id}.json?\"\n",
        "res = requests.get(url, headers=headers)\n",
        "\n",
        "import json\n",
        "print(json.dumps(res.json(), indent=2))"
      ],
      "metadata": {
        "id": "yiwQHbrp3YZn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}