{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDkEg/VdBABJFIy0CzOCHH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flari-gold/IU/blob/main/IU_RedditDataAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#package installs with pip\n",
        "!pip install praw\n",
        "!pip install HanTa\n",
        "\n",
        "#import modules\n",
        "from google.colab import userdata\n",
        "import praw\n",
        "import collections\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from HanTa import HanoverTagger"
      ],
      "metadata": {
        "id": "TvJ-SwMePddJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter\n",
        "#Daten der unter https://old.reddit.com/prefs/apps/ generierten Anwendung. Ausreichend für ReadOnly Zugriffe.\n",
        "#Die Daten sind im Google Colab Secrets hinterlegt\n",
        "CLIENT_ID=userdata.get('CLIENT_ID')\n",
        "CLIENT_SECRET=userdata.get('CLIENT_SECRET')\n",
        "\n",
        "#Für Write Zugriffe muss ebenfalls noch der eigene Account angegeben werden, kann in diesem Fall ignoriert werden\n",
        "#USERNAME=userdata.get('USERNAME')\n",
        "#PASSWORD=userdata.get('PASSWORD')\n",
        "\n",
        "#Parameter für die Datenanalyse\n",
        "SUBREDDIT='Sylosis'\n",
        "ANZAHL_POSTS=100\n",
        "TOPICS=5\n",
        "\n",
        "#initialisierung reddit API\n",
        "reddit = praw.Reddit(\n",
        "    client_id=CLIENT_ID,\n",
        "    client_secret=CLIENT_SECRET,\n",
        "    user_agent=\"Mein Topic Modeling Script\", #User Agent ist zwingend notwendig\n",
        "    check_for_async=False #PRAW wirft eine Warnung welche die ASYNC API empfiehlt, kann ignoriert werden\n",
        ")"
      ],
      "metadata": {
        "id": "aFH0pne4ODPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#declaration\n",
        "all_textdata = []\n",
        "user_upvotes = collections.Counter()\n",
        "user_contributionscounter = collections.Counter()\n",
        "\n",
        "#Anfragen an die Reddit API mit über request liefern die Daten im JSON Format. PRAW vereinfacht dies, in dem man die direkt die Attribute ansprechen kann\n",
        "for submission in reddit.subreddit(SUBREDDIT).hot(limit=ANZAHL_POSTS):\n",
        "\n",
        "    user_contributionscounter[submission.author] += 1\n",
        "    user_upvotes[submission.author] += submission.score\n",
        "\n",
        "    post_textdata = [submission.title]#Alle Reddit Beiträge haben einen Titel. Der Titel wird ebenfalls im Topic Modeling wie ein normaler Kommentar behandelt\n",
        "    post_textdata.append(submission.selftext)#Nicht alle Beträge haben einen Selftext. Beiträge die auf Links verweisen beispielsweise. Falls Selftext vorhanden ist, wird er im Topic Modeling behandelt, ansonsten ist der String leer\n",
        "\n",
        "#Sobald ein Kommntar auf Reddit zu viele Antworten hat, werden die Antworten hinter einer Schaltfläche \"more\" verborgen und das gleiche gilt auch beim Zugriff auf die API.\n",
        "#Das Auflösen der \"more\" stellt einen eigenen API Aufruf da und sorgt aufgrund der Zugriff Limits bei größeren Anfragen zu einer langen Laufzeit\n",
        "    submission.comments.replace_more(limit=None)\n",
        "    for comment in submission.comments.list():\n",
        "        post_textdata.append(comment.body)\n",
        "        user_contributionscounter[comment.author] += 1\n",
        "        user_upvotes[comment.author] += comment.score\n",
        "\n",
        "    all_textdata.append(post_textdata)\n",
        "\n",
        "\n",
        "print(user_contributionscounter)\n",
        "print(user_upvotes)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZ7Ks5C0Pl3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean text\n",
        "all_cleantext = []\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['take', 'use', 'would', 'get'])\n",
        "\n",
        "tagger_en = HanoverTagger.HanoverTagger('morphmodel_en.pgz')\n",
        "#Vllt noch ändern, Slipknot und Slipknots z.B. sollte schon gelemmat werden\n",
        "tagger_Exclude = ['sylosis'] #Beim Testen von Subreddits von Musik Bands kommt es mit dem Tagger zu Problemen\n",
        "\n",
        "for i in all_textdata:\n",
        "    string_result = []\n",
        "    for j in i:\n",
        "        templist = []\n",
        "        for word in j.lower().split():\n",
        "          if word not in tagger_Exclude:\n",
        "            templist.append(tagger_en.analyze(word)[0])\n",
        "          else:\n",
        "            templist.append(word)\n",
        "        templist = [word for word in templist if word not in stop_words]\n",
        "        tempstring = ' '.join(templist)\n",
        "        string_result.append(tempstring.replace(\".\", \"\").replace(\",\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\"(\", \"\").replace(\")\", \"\"))\n",
        "    all_cleantext.append(string_result)\n",
        "    #print(string_result)\n",
        "\n"
      ],
      "metadata": {
        "id": "ptDocgDFQRpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "#all_cleantext ist eine Liste, welche Listen beinhaltet, welche wiederum Strings beinhalten\n",
        "#Jeder String repräsentiert einen Post/Kommentar\n",
        "#Für die Verarbeitung muss aber in der Liste für jedes Wort ein eigener String stehen\n",
        "templist = []\n",
        "for i in all_cleantext:\n",
        "    long_string = ' '.join(i)\n",
        "    templist.append(long_string.split())\n",
        "\n",
        "# Create dictionary and corpus\n",
        "dictionary = corpora.Dictionary(templist)\n",
        "corpus = [dictionary.doc2bow(text) for text in templist]\n",
        "\n",
        "# Train LDA model\n",
        "lda_model = LdaModel(corpus, num_topics=TOPICS, id2word=dictionary, passes=10)\n",
        "\n",
        "from pprint import pprint\n",
        "pprint(lda_model.print_topics())\n"
      ],
      "metadata": {
        "id": "rdiN6Pv2JWmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyLDAvis"
      ],
      "metadata": {
        "id": "afAGKD3U6EwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "# Visualisierung\n",
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(panel)"
      ],
      "metadata": {
        "id": "9PjkrBj-6GLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.corpora as corpora# Create Dictionary\n",
        "\n",
        "templist = []\n",
        "for i in all_cleantext:\n",
        "    long_string = ' '.join(i)\n",
        "    templist.append(long_string.split())\n",
        "\n",
        "\n",
        "id2word = corpora.Dictionary(templist)# Create Corpus\n",
        "texts = templist# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]# View\n",
        "#print(corpus[:1][0][:30])\n",
        "\n",
        "from pprint import pprint\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word, num_topics=TOPICS, passes=10)\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "#doc_lda = lda_model[corpus]\n",
        "#print(lda_model.get_topics())"
      ],
      "metadata": {
        "id": "q6GmSyN262W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "#all_cleantext ist eine Liste, welche Listen beinhaltet, welche wiederum Strings beinhalten\n",
        "#Jeder String repräsentiert einen Post/Kommentar\n",
        "#Für die Verarbeitung muss aber in einer Liste Strings stehen\n",
        "templist = []\n",
        "for i in all_cleantext:\n",
        "    long_string = ' '.join(i)\n",
        "    templist.append(long_string)\n",
        "\n",
        "# Text data\n",
        "\n",
        "# Create document-term matrix\n",
        "vectorizer = CountVectorizer()\n",
        "dtm = vectorizer.fit_transform(templist)\n",
        "\n",
        "# Train LDA model\n",
        "lda_model = LatentDirichletAllocation(n_components=TOPICS)\n",
        "lda_model.fit(dtm)\n",
        "\n",
        "import numpy as np\n",
        "words = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Top-Wörter für jedes Thema\n",
        "n_top_words = 5\n",
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "    print(f\"Topic #{topic_idx + 1}:\")\n",
        "    print(\" \".join(words[np.argsort(topic)[-n_top_words:]]))\n",
        "\n",
        "#doc_topic_distributions = lda_model.transform(dtm)\n",
        "\n",
        "# Ausgabe\n",
        "#for doc_idx, topic_dist in enumerate(doc_topic_distributions):\n",
        "#    print(f\"Document #{doc_idx + 1}:\")\n",
        "#    print(f\"Topic Distribution: {topic_dist}\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Top-Wörter pro Thema als DataFrame\n",
        "topic_word_df = pd.DataFrame(\n",
        "    lda_model.components_,\n",
        "    columns=vectorizer.get_feature_names_out()\n",
        ")\n",
        "#print(topic_word_df)\n",
        "\n",
        "doc_topic_df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i+1}\" for i in range(lda_model.n_components)])\n",
        "print(doc_topic_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "7mNMWXR2MSiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "templist = []\n",
        "for i in all_cleantext:\n",
        "    long_string = ' '.join(i)\n",
        "    templist.append(long_string)\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf = True, max_features = 5, smooth_idf=True)\n",
        "#model = vectorizer.fit_transform([templist[0], templist[1], templist[2]])\n",
        "model = vectorizer.fit_transform(templist)\n",
        "\n",
        "\n",
        "lda_model=LatentDirichletAllocation(n_components=TOPICS,learning_method='online')\n",
        "lda_top=lda_model.fit_transform(model)\n",
        "\n",
        "for i,topic in enumerate(lda_top[0]):\n",
        "\tprint(\"Topic \",i,\": \",topic*100,\"%\")\n",
        "\n",
        "for i,topic in enumerate(lda_top[1]):\n",
        "\tprint(\"Topic \",i,\": \",topic*100,\"%\")\n",
        "\n",
        "for i,topic in enumerate(lda_top[2]):\n",
        "\tprint(\"Topic \",i,\": \",topic*100,\"%\")\n"
      ],
      "metadata": {
        "id": "XqZ02Xnuf5UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFrtKDCf5Qsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "templist = []\n",
        "for i in all_cleantext:\n",
        "    long_string = ' '.join(i)\n",
        "    templist.append(long_string)\n",
        "end_string = ' '.join(templist)\n",
        "\n",
        "\n",
        "print(collections.Counter(end_string.split()).most_common(10))"
      ],
      "metadata": {
        "id": "2X_KmyZ36FFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wHL8rR-R5TXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "templist = []\n",
        "for i in all_cleantext:\n",
        "    long_string = ' '.join(i)\n",
        "    templist.append(long_string)\n",
        "end_string = ' '.join(templist)\n",
        "print(end_string)\n",
        "\n",
        "#\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')# Generate a word cloud\n",
        "wordcloud.generate(long_string)# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "KVht1ejpxa8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CrQchaSYbroy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.linspace(0, 10, 100)\n",
        "\n",
        "plt.plot(x, np.sin(x))\n",
        "plt.plot(x, np.cos(x))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DKG3GTYD8yRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install HanTa\n",
        "from HanTa import HanoverTagger\n",
        "\n",
        "tagger_en = HanoverTagger.HanoverTagger('morphmodel_en.pgz')\n",
        "\n",
        "print(tagger_en.analyze('driving'))\n",
        "print(type(tagger_en.analyze('driving')))\n",
        "print(tagger_en.analyze('driving')[0])"
      ],
      "metadata": {
        "id": "R8RGAmbv166c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}